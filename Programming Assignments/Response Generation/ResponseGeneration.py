# -*- coding: utf-8 -*-
"""RespGen

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kKQeMnU0acOWL_XAGSBgVqEY2-n8ZWq6

# Data Loading and Generation
"""

import os
if "aesthetix.py" not in os.popen("ls").read():
  !wget https://raw.githubusercontent.com/Stellarator-X/ConvoBot/servus/Programming%20Assignments/Speech%20Recognition/ds_utils/aesthetix.py
import numpy as np 
from sklearn.model_selection import train_test_split
import aesthetix as at

# Downloading movie_lines
if "movie_lines.txt" not in os.popen("ls").read():
  !wget https://raw.githubusercontent.com/Stellarator-X/ConvoBot/servus/Programming%20Assignments/Response%20Generation/movie_lines.txt

movielines = open("movie_lines.txt", mode='r')
print(movielines)
lines = movielines.readlines()
print(len(lines))

def clean_str(_str):
  _str = _str.strip()
  _str = _str.lower()
  _str = _str.replace(".", "")
  _str = _str.replace(",", "")
  _str = _str.replace("?", "")
  _str = _str.replace("!", "")
  _str = _str.replace(":", "")
  _str = _str.replace("-", " ")
  _str = _str.replace("_", " ")
  _str = _str.replace("\\", "")
  _str = _str.replace("  ", " ")
  return _str

sample_size = 20000
cleanlines = []
for i, line in enumerate(lines[:sample_size]):
  at.progress_bar("Cleaning the lines", i, len(lines[:sample_size]))
  speaker, line = line.split('+++$+++ ')[-2:]
  cleanlines.append([speaker.split(" ")[0], line.split('\n')[0]])

cleanlines.reverse()
cleanlines = np.array(cleanlines)
for line in cleanlines[:10]:
  print(line[0],":",line[1])


# Forming the dataset 
response_data = []
l = len(cleanlines)-1
for i, line in enumerate(cleanlines[:-1]):
  at.progress_bar("Generating Stimulus-Response Pairs", i, l)
  speaker, utterance = line
  next_speaker, next_utterance = cleanlines[i+1]
  if speaker is not next_speaker:
    response_data.append(np.array(["<start> "+clean_str(utterance)+" <end>", "<start> "+clean_str(next_utterance)+" <end>"]))
  
response_data = np.array(response_data)
print(response_data.shape)
print(response_data[-10:])

# import pandas as pd
# a = pd.DataFrame({'Context': response_data[:, 0], 'Responses': response_data[:, 1]})
# a.to_csv("Dat.csv")

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt 
# %matplotlib inline
a = [len(s) for s in response_data[:,0]]
# a_ = [s for s in response_data[:,0] if len(s)>2000]
plt.plot(a)
plt.show()
print(np.mean(np.array(a)))
print("Longest utterance : ", response_data[np.argmax(a), 0])

# Preprocessing the Data
from tensorflow.keras.preprocessing.text import Tokenizer 
from tensorflow.keras.preprocessing.sequence import pad_sequences

oov_token = "<OOV>"
max_length = 40
stimuli = response_data[:, 0]
responses = response_data[:, 1]

tokenizer = Tokenizer(oov_token=oov_token)
tokenizer.fit_on_texts(stimuli)

word_index = tokenizer.word_index
word_index['<start>'] = 0
word_index['<end>'] = len(word_index)+1
index_word = {word_index[word]:word for word in word_index}
vocab_size = len(word_index)
stimulus_sequences = tokenizer.texts_to_sequences(stimuli)
response_sequences = tokenizer.texts_to_sequences(responses)

padded_stimulus_sequences = pad_sequences(stimulus_sequences, maxlen = max_length ,padding = 'post', truncating = 'post')
padded_response_sequences = pad_sequences(response_sequences, maxlen = max_length, padding = 'post', truncating = 'post')

index_word[1829]

# Getting the embedding weights from pre-trained glove vectors
embedding_dim = 100

!wget --no-check-certificate \
    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \
    -O /tmp/glove.6B.100d.txt
embeddings_index = {};
with open('/tmp/glove.6B.100d.txt') as f:
    for line in f:
        values = line.split();
        word = values[0];
        coefs = np.asarray(values[1:], dtype='float32');
        embeddings_index[word] = coefs;

embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));
for word, i in word_index.items():
    embedding_vector = embeddings_index.get(word);
    if embedding_vector is not None:
        embeddings_matrix[i] = embedding_vector;

"""# Encoder-Decoder with Attention"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import matplotlib.pyplot as plt
import tensorflow as tf 
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Dense, Bidirectional, Embedding, LSTM, RepeatVector, Concatenate, Dot, Activation, Input
from tensorflow.keras import backend as K
import tensorflow_addons as tfa
print(tf.__version__)
# %matplotlib inline

BATCH_SIZE = 64
BUFFER_SIZE = len(padded_response_sequences)
steps_per_epoch = BUFFER_SIZE//BATCH_SIZE
embedding_dims = 256
rnn_units = 1024
dense_units = 1024
Dtype = tf.float32   #used to initialize DecoderCell Zero state
Tx = max_length
Ty = max_length

# Initialising the dataset
dataset = tf.data.Dataset.from_tensor_slices((padded_stimulus_sequences, padded_response_sequences)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
example_X, example_Y = next(iter(dataset))
print(example_X.shape) 
print(example_Y.shape)

class EncoderNet(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units, embeddings_matrix):
    super(EncoderNet, self).__init__()
    self.encoder_embedding = tf.keras.layers.Embedding(input_dim = vocab_size+1,
                                                       output_dim = embedding_dim,
                                                       weights = [embeddings_matrix], trainable = False)
    self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences = True, return_state = True)
  
# Decoder
class DecoderNet(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units, embeddings_matrix):
    super(DecoderNet, self).__init__()
    self.decoder_embedding = tf.keras.layers.Embedding(input_dim = vocab_size+1,
                                                       output_dim = embedding_dim, weights = [embeddings_matrix], trainable= False)
    self.dense_layer = Dense(vocab_size)
    self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)
    #Sampler
    self.sampler = tfa.seq2seq.sampler.TrainingSampler()

    self.attention_mechanism = self.build_attention_mechanism(dense_units, None, BATCH_SIZE*[Tx])
    self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)
    self.decoder  = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler = self.sampler, output_layer = self.dense_layer)
  
  def build_attention_mechanism(self, units, memory, memory_sequence_length):
    return tfa.seq2seq.LuongAttention(units, memory= memory, memory_sequence_length=memory_sequence_length)
  
  def build_rnn_cell(self, batch_size):
    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism, attention_layer_size = dense_units)
    return rnn_cell
  
  def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):
    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, dtype = Dtype)
    decoder_initial_state = decoder_initial_state.clone(cell_state =encoder_state)
    return decoder_initial_state

encoderNet = EncoderNet(vocab_size, embedding_dim, rnn_units, embeddings_matrix)
decoderNet = DecoderNet(vocab_size, embedding_dim, rnn_units, embeddings_matrix)

optimizer = tf.keras.optimizers.Adam()

def loss_function(y_pred, y):
  #shape of y [batch_size, ty]
  #shape of y_pred [batch_size, Ty, output_vocab_size] 
  sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,
                                                                                reduction='none')
  loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)
  #skip loss calculation for padding sequences i.e. y = 0 
  
  # mask the loss when padding sequence appears in the output sequence
  mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1
  mask = tf.cast(mask, dtype=loss.dtype)
  loss = mask* loss
  loss = tf.reduce_mean(loss)
  return loss

decoderNet.attention_mechanism.memory_initialized

def train_step(input_batch, output_batch, encoder_initial_cell_state):
  loss = 0
  with tf.GradientTape() as tape:
      encoder_emb_inp = encoderNet.encoder_embedding(input_batch)
      a, a_tx, c_tx = encoderNet.encoder_rnnlayer(encoder_emb_inp, 
                                                      initial_state =encoder_initial_cell_state)

      #[last step activations,last memory_state] of encoder passed as input to decoder Network
      
        
      # Prepare correct Decoder input & output sequence data
      decoder_input = output_batch[:,:-1] # ignore <end>
      #compare logits with timestepped +1 version of decoder_input
      decoder_output = output_batch[:,1:] #ignore <start>


      # Decoder Embeddings
      decoder_emb_inp = decoderNet.decoder_embedding(decoder_input)

      #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState
      decoderNet.attention_mechanism.setup_memory(a)
      decoder_initial_state = decoderNet.build_decoder_initial_state(BATCH_SIZE,
                                                                          encoder_state=[a_tx, c_tx],
                                                                          Dtype=tf.float32)
      
      #BasicDecoderOutput        
      outputs, _, _ = decoderNet.decoder(decoder_emb_inp,initial_state=decoder_initial_state,
                                              sequence_length=BATCH_SIZE*[Ty-1])

      logits = outputs.rnn_output
      #Calculate loss

      loss = loss_function(logits, decoder_output)

  #Returns the list of all layer variables / weights.
  variables = encoderNet.trainable_variables + decoderNet.trainable_variables  
  # differentiate loss wrt variables
  gradients = tape.gradient(loss, variables)

  #grads_and_vars â€“ List of(gradient, variable) pairs.
  grads_and_vars = zip(gradients,variables)
  optimizer.apply_gradients(grads_and_vars)
  return loss

# from google.colab import drive
# drive.mount('/content/drive', force_remount=True )

# # Training
# checkpointdir = os.path.join('/content/drive/My Drive/DL',"ResponseGen")
# chkpoint_prefix = os.path.join(checkpointdir, "chkpoint")
# if not os.path.exists(checkpointdir):
#     os.mkdir(checkpointdir)

# checkpoint = tf.train.Checkpoint(optimizer = optimizer, encoderNetwork = encoderNetwork, 
#                                  decoderNetwork = decoderNetwork)

# try:
#     status = checkpoint.restore(tf.train.latest_checkpoint(checkpointdir))
#     print("Checkpoint found at {}".format(tf.train.latest_checkpoint(checkpointdir)))
# except:
#     print("No checkpoint found at {}".format(checkpointdir))

def initialize_initial_state():
        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]

vocab_size

epochs = 50
print("Training :")
for i in range(1, epochs+1):
    encoder_initial_cell_state = initialize_initial_state()
    total_loss = 0.0
    disp_loss = 0.0
    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):
        at.progress_bar(f"Epoch {i}/{epochs}", batch, steps_per_epoch, output_vals = {'Loss' : disp_loss})
        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)
        total_loss += batch_loss
        disp_loss = total_loss/(max(batch, 1))

index_word[1829]

"""# Beam Search"""

beam_width = 3

sample_lines = ["Hello","Have you trained the Model?", "who are you"]
sample_lines = [clean_str(line) for line in sample_lines]

sample_sequences  = tokenizer.texts_to_sequences(sample_lines)
padded_sample_sequences = pad_sequences(sample_sequences, max_length, padding = "post", truncating = 'post')

input_ = tf.convert_to_tensor(padded_sample_sequences)
input_batch_size = len(sample_lines)
print(input_.shape)

encoder_init_state = [tf.zeros((input_batch_size, rnn_units))]*2
encoder_embedding_input = encoderNet.encoder_embedding(input_)

a, a_Tx, c_Tx = encoderNet.encoder_rnnlayer(encoder_embedding_input, initial_state = encoder_init_state)

start_tokens = tf.fill([input_batch_size], word_index['<start>'])

end_token = word_index['<end>']

decoder_input = tf.expand_dims([word_index['<start>']]*input_batch_size, 1)
decoder_embedding_input = decoderNet.decoder_embedding(decoder_input)

# As per implementation necessities
encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)
decoderNet.attention_mechanism.setup_memory(encoder_memory)
print(f"encoder_memory.shape = {encoder_memory.shape}")

decoder_init_state = decoderNet.rnn_cell.get_initial_state(batch_size = input_batch_size*beam_width, dtype = Dtype)
encoder_state = tfa.seq2seq.tile_batch([a_Tx, c_Tx], multiplier=beam_width)
decoder_init_state = decoder_init_state.clone(cell_state=encoder_state) 

decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNet.rnn_cell,beam_width=beam_width,
                                                 output_layer=decoderNet.dense_layer)

max_iterations = max_length
decoder_embedding_matrix = decoderNet.decoder_embedding.variables[0] 
first_fin, first_inp, first_state = decoder_instance.initialize(embedding=decoder_embedding_matrix, start_tokens = start_tokens, end_token = end_token, initial_state = decoder_init_state)
print("\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :",first_inp.shape)

inputs = first_inp
state = first_state
predictions = np.empty((input_batch_size, beam_width,0), dtype = np.int32)
beam_scores =  np.empty((input_batch_size, beam_width,0), dtype = np.float32)                                                                            
for j in range(max_iterations):
  beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)
  inputs = next_inputs
  state = next_state
  outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)
  scores = np.expand_dims(beam_search_outputs.scores,axis = -1)
  predictions = np.append(predictions, outputs, axis = -1)
  beam_scores = np.append(beam_scores, scores, axis = -1)
print(predictions.shape) 
print(beam_scores.shape)

import itertools
print("Stimuli:")
for line in sample_lines :
  print(line)
print("Responses")
for i in range(len(predictions)):
  output_beams_per_sample = predictions[i,:,:]
  score_beams_per_sample = beam_scores[i,:,:]
  for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :
      seq = list(itertools.takewhile( lambda index: index !=2, beam))
      score_indexes = np.arange(len(seq))
      beam_score = score[score_indexes].sum()
      print(" ".join( [index_word[w] for w in seq]), " beam score: ", beam_score)

